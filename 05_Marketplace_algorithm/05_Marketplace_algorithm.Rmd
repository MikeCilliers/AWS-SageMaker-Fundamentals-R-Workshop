---
title:  "SageMaker fundamentals for R users - Part 04: AutoML with AutoGluon-Tabular"
output: html_notebook
---


We did not specify separate validation data and so AutoGluon automatically choses a random training/validation split of the data. The data used for validation is seperated from the training data and is used to determine the models and hyperparameter-values that produce the best results

The first line of your CSV file should contain names for each column.
Columns in your CSV file can be strings/text-fields/Numeric.
Your data must contain the column that you identify as 'label' in your hyperparameter configuration

For best results: provide all your data to AutoGluon as train_data rather than
splitting a validation set yourself, specify which eval_metric will be used to
evaluate predictions.

If you donâ€™t have a strong reason to provide your own validation dataset, we recommend you omit the tuning_data argument. This lets AutoGluon automatically select validation data from your provided training set (it uses smart strategies such as stratified sampling). 



## A quick introduction to AutoGluon-Tabular


## Load necessary libraries and activate conda environment

```{r}
library(readr)        # for importing the previously saved and preprocessed data and saving a new training set to disk
library(dplyr)        # for preprocessing the data
library(reticulate)   # for calling the SageMaker Python SDK from R
library(purrr)        # primarily for parsing the SageMaker responses
library(pROC)         # for the evaluation of the final model performance
library(caret)        # for the evaluation of the final model performance
```

## Preparation

```{r}
use_condaenv("sagemaker-r", required = TRUE)

sagemaker <- import("sagemaker")
session <- sagemaker$Session()
```

```{r}
bucket <- session$default_bucket()
project <- "hotels"
data_path <- paste0("s3://", bucket, "/", project, "/", "data")
models_path <- paste0("s3://", bucket, "/", project, "/", "models")
```

```{r}
table_header <- read_csv("../data/table_header.csv") %>% names()
hotels_training  <- read_csv("../data/hotels_training.csv", col_names = table_header)
hotels_validation <- read_csv("../data/hotels_validation.csv", col_names = table_header)

training_combined <- bind_rows(hotels_training, hotels_validation)
rm(training_set, validation_set)

write_csv(training_combined, "../data/hotels_training_autogluon.csv")
```
`

```{r}
s3_uploader <- sagemaker$s3$S3Uploader()

s3_train <- s3_uploader$upload(local_path = "../data/hotels_training_autogluon.csv", 
                               desired_s3_uri = data_path)
```


## Training the AutoGluon model

### Step 1 - Create an Estimator object


```{r}
region <- session$boto_region_name

# get SageMaker execution role stored in .Renviron
role_arn <- Sys.getenv("SAGEMAKER_ROLE_ARN")

autogluon_tab_estimator <- sagemaker$AlgorithmEstimator(
  algorithm_arn = "arn:aws:sagemaker:us-east-1:865070037744:algorithm/autogluon-tabular-v3-4-b164f6c99d3e4ca2b6b56e72df06a4f2", 
  role = role_arn, 
  instance_count = 1L, 
  instance_type = 'ml.m5.12xlarge', 
  volume_size = 100L,
  output_path =  models_path, 
  sagemaker_session = session)
```



### Step 2 - Define the static hyperparameters

Compared to XGBoost not a lot hyperparameters

```{r}
fit_args = list(eval_metric = "roc_auc", label = "children", presets = "best_quality")

autogluon_tab_estimator$set_hyperparameters(
  fit_args = fit_args, 
  feature_importance = TRUE
)
```


### Step 3 - Define the S3 location of the data sets and the training job name

```{r}
# Create training job name based project organization principles
algo <- "autogluon-tabular"
timestamp <- format(Sys.time(), "%Y-%m-%d-%H-%M-%S")
job_name <- paste(project, algo, timestamp, sep = "-")

s3_input <- sagemaker$inputs$TrainingInput(s3_data = s3_train,
                                     content_type = 'csv')

input_data <- list('training' = s3_input)
```


### Step 4 - Start the training job

```{r}
autogluon_tab_estimator$fit(inputs = input_data,
                  job_name = job_name,
                  wait = FALSE  # If set to TRUE, the call will wait until the job completes
                  )
```


```{r}
session$describe_training_job(job_name)[["TrainingJobStatus"]]
```

### Step 5 - Evaluate the training results

```{r}
training_job_stats <- session$describe_training_job(job_name = job_name)

final_metrics <-  map_df(training_job_stats$FinalMetricDataList, 
                          ~tibble(metric_name = .x[["MetricName"]],
                                  value = .x[["Value"]]))
final_metrics
```







## Final model evaluation

### Step 1 - Create a Transformer object

Note
If you subscribe to an algorithm on AWS Marketplace, you must create a model package before you can use it to get inferences by creating hosted endpoint or running a batch transform job.

Consuming SageMaker Model Packages
SageMaker Model Packages are a way to specify and share information for how to create SageMaker Models. With a SageMaker Model Package that you have created or subscribed to in the AWS Marketplace, you can use the specified serving image and model data for Endpoints and Batch Transform jobs.

To work with a SageMaker Model Package, use the ModelPackage class.


https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-mkt-buy.html

OPTION 1

```{r}
predictions_path <- paste0(models_path, "/", job_name, "/predictions")

model_location <- session$describe_training_job(job_name)[["ModelArtifacts"]][["S3ModelArtifacts"]]

session$create_model_package_from_algorithm(name = "autogluon-package3", description = "ag2", algorithm_arn = "arn:aws:sagemaker:us-east-1:865070037744:algorithm/autogluon-tabular-v3-4-b164f6c99d3e4ca2b6b56e72df06a4f2", model_data = model_location )

session$create_model_package_from_algorithm(
  name = "late-night-package",
  description = paste0("Model Package created from training with ", Sys.getenv("AUTOGLUON_TABULAR")),
  algorithm_arn = Sys.getenv("AUTOGLUON_TABULAR"),
  model_data = model_location)

model <- sagemaker$ModelPackage(role = role_arn, 
                                model_package_arn = "arn:aws:sagemaker:us-east-1:198562456021:model-package/late-night-package")

# How to retrieve the model_package_arn via the SageMaker API?


model <- sagemaker$ModelPackage(role = role_arn, model_package_arn = "arn:aws:sagemaker:us-east-1:198562456021:model-package/autogluon-package")

model_2 <- sagemaker$ModelPackage(role = role_arn, model_package_arn = "arn:aws:sagemaker:us-east-1:198562456021:model-package/autogluon-package2")
```


```{r}
env_list <- list(
  SAGEMAKER_PROGRAM = "inference.py",
  SAGEMAKER_SUBMIT_DIRECTORY = "/opt/ml/model/code", 
  SAGEMAKER_CONTAINER_LOG_LEVEL = 20L,
  SAGEMAKER_REGION = region
)
```



```{r}
# this call creates the model. Convention: model name = model package name + server time stamp

autogluon_batch_predictor <- model$transformer(
  instance_count = 1L, 
  instance_type = "ml.m5.4xlarge", 
  strategy = "MultiRecord",
  env = env_list,
  #max_payload = 1L, 
  max_concurrent_transforms = 1L, 
  output_path = predictions_path)
```




```{r}
job_name <- "hotels-autogluon-tabular-2021-02-27-21-58-38"
job_name <- paste0(job_name, "-latenightfun2")

# Add job status
autogluon_batch_predictor$transform(
  data = s3_test,
  content_type = "text/csv",
  split_type = "Line",
  job_name = job_name,
  wait = FALSE
)
```

```{r}
session$describe_transform_job(job_name)[["TransformJobStatus"]]
```

OPTION 2


### Step 1 - Create a Transformer object

```{r}
job_name_mod <- paste0(job_name, 4)
```

```{r}
env_list <- list(
  SAGEMAKER_PROGRAM = "inference.py",
  SAGEMAKER_SUBMIT_DIRECTORY = "/opt/ml/model/code", 
  SAGEMAKER_CONTAINER_LOG_LEVEL = 20L,
  SAGEMAKER_REGION = region
)
```


```{r}
predictions_path <- paste0(models_path, "/", job_name_mod, "/predictions")

autogluon_batch_predictor <- autogluon_tab_estimator$transformer(
  instance_count = 1L, 
  instance_type = "ml.m5.4xlarge", 
  strategy = "MultiRecord", 
  env = env_list,
  #max_payload = 1L, 
  max_concurrent_transforms = 1L, 
  output_path = predictions_path
)



```


Insights:
Call above created model package + model

model package has link to inference image?


### Step 2 - Start the batch prediction job

```{r}
autogluon_batch_predictor$transform(
  data = s3_test,
  content_type = "text/csv",
  split_type = "Line",
  job_name = job_name_mod,
  wait = FALSE
)
```





```{r}

job_name_mod <- "hotels-autogluon-tabular-2021-02-27-21-58-383"

predictions_path <- paste0(models_path, "/", job_name_mod, "/predictions")


session$describe_transform_job(job_name_mod)[["TransformJobStatus"]]
```


### Step 3 - Download the test set predictions


```{r}
s3_downloader <- sagemaker$s3$S3Downloader()
s3_test_predictions_path <- s3_downloader$list(predictions_path)
 
dir.create("./predictions")
s3_downloader$download(s3_test_predictions_path, "./predictions")
 
test_predictions <- read_csv("./predictions/hotels_test_with_header_without_dependent.csv.out",
                              col_names = FALSE) %>% 
   pull(X1)
```

```{r}
## Need to think about this code chunk later
hotels_test <- read_csv("../data/hotels_test_with_dependent_variable.csv")
```






```{r}
test_results <- tibble(
  truth = hotels_test$children,
  predictions = test_predictions
)

head(test_results)

```

### Step 4 - Evaluate the test set predictions

```{r}
roc_obj <- roc(test_results$truth,test_results$predictions,
               plot = TRUE,         
               grid = TRUE,
               print.auc = TRUE,
               legacy.axes = TRUE, 
               main = "ROC curve for XGBoost classification",
               show.thres=TRUE,
               col = "red2"
)
```




```{r}
conf_matrix <- confusionMatrix(
  factor(ifelse(test_results$predictions >= 0.5, 1, 0), levels = c("0", "1"), 
         labels = c("no children", "children")),
  factor(test_results$truth, levels = c(0, 1), 
         labels = c("no children", "children")),
  positive = "children")
conf_matrix
```


## Summary
